<h2 style="text-align:center"> Previous Talks </h2>

<div class="talks">
  <!-- Chaowei -->
  <div class="talk" id="chaowei">
        <div class="speakerInfo"> 
            <img alt="Chaowei Xiao" src="{{site.baseurl}}/assets/img/chaowei.jpg">
      <br>
      <a href="http://www-personal.umich.edu/~xiaocw/" target="_blank">Chaowei Xiao</a> 
      <br>
      University of Michigan, NVIDIA Research
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, November 30th, 2020 @ 2PM.
      <br>
<strong> Talk Title: </strong> Deep Learning in Adversarial Environments
     <br>
      <strong> Bio: </strong> Chaowei Xiao is a research scientist at NVIDIA Research. He earned his Ph.D. at the University of Michigan, working with Professor Mingyan Liu. His research interests lie at the intersection of computer  security,  privacy,  and machine  learning. His works have been featured in multiple media outlets, including Wired, Fortune, IEEE SPECTRUM. One of his research outputs is now on display at the Science Museum in London.  He has received the best paper award at Mobicom 2014.
    </div>
  </div>
    <!-- Aws -->
  <div class="talk" id="aws">
        <div class="speakerInfo"> 
            <img alt="Aws Albarghouthi" src="{{site.baseurl}}/assets/img/aws.jpg">
      <br>
      <a href="http://pages.cs.wisc.edu/~aws/" target="_blank">Aws Albarghouthi</a> 
      <br>
      <a href="https://www.wisc.edu/" target="_blank">University of Wisconsin-Madison</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, December 14th, 2020 @ 4PM.
      <br>
<strong> Talk Title: </strong> A Tale of Two Applications of Verification in Machine Learning
     <br>
      <strong> Abstract: </strong> The rise of machine learning, particularly in the form of deep learning, has created a qualitative shift in our conception of what software is and what software can accomplish. But, of course, it’s not all rainbows and butterflies. Researchers have been hard at work trying to understand the fragility of the machine-learning pipeline: from training to inference, small changes to the input can result in radical changes to the output, which can lead to security, safety, as well as ethical problems. In this talk, I will show how new techniques from software verification can help us reason about, and ensure, robustness of machine-learning techniques against training-time (poisoning) and test-time (adversarial-example) attacks.
<br/>
This talk is based on joint work with Yuhao Zhang, Samuel Drews, and Loris D’Antoni.
      <br>
      <strong> Bio: </strong> Aws Albarghouthi is an assistant professor at the University of Wisconsin-Madison. He studies automated synthesis and verification of programs. He received his PhD from the University of Toronto in 2015. He has received a number of best-paper awards for his work (at FSE, UIST, and FAST), an NSF CAREER award, a Google Faculty Research Award, and Facebook Research Awards. Aws is very excited about his virtual visit to Waterloo.
    </div>
    <br>
    <iframe width="100%" src="https://www.youtube-nocookie.com/embed/9CSc0dbL2V4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>
    <!-- Aditya and Matthew -->
  <div class="talk" id="aditya">
    <div class="speakerInfo"> 
      <img alt="Aditya Thakur" src="{{site.baseurl}}/assets/img/aditya.jpg">
      <br>
      <a href="http://thakur.cs.ucdavis.edu/" target="_blank">Aditya Thakur</a> 
      <br>
      <a href="https://www.ucdavis.edu/" target="_blank">University of California, Davis</a>
      <br>
      <img alt="Matthew Ali Sotoudeh" src="{{site.baseurl}}/assets/img/sotoudeh.jpg">
      <br>
      <a href="https://masot.net/" target="_blank">Matthew Sotoudeh</a> 
      <br>
      <a href="https://www.ucdavis.edu/" target="_blank">University of California, Davis</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, December 21st, 2020 @ 4PM.
      <br>
<strong> Talk Title: </strong> Understanding and Repairing Deep Neural Networks
     <br>
      <strong> Abstract: </strong> Deep neural networks (DNNs) have been successfully applied to a wide variety of problems, including image recognition, natural-language processing, medical diagnosis, and self-driving cars. As the accuracy of DNNs has increased so has their complexity and size, making the outputs of such models difficult to meaningfully interpret. The talk describes a new symbolic representation for DNNs that allowed us to exactly compute the integrated gradients, a state-of-the-art network attribution method that until now has only been approximated.

Moreover, DNNs are far from infallible, and mistakes made by DNNs have led to loss of life, motivating research on verification and testing to find mistakes in DNNs. In contrast, the talk describes techniques and tools for repairing a trained DNN once a mistake has been discovered. We present Provable Repair of DNNs, which computes a minimal change to the parameters of a trained DNN to correct its behavior according to a given specification, and ensures that the patch is provably effective, generalizing, local, and efficient.
      <br>
      <strong>Aditya Thakur's Bio: </strong> Aditya Thakur is an assistant professor of Computer Science at the University of California, Davis. He received his Ph.D. from the University of Wisconsin, Madison, and has held positions at Google, Microsoft Research, and the University of California, Berkeley. His research interests include programming languages, machine learning, formal methods, and software engineering. He was the recipient of the Facebook Probability and Programming Research Award 2019 and 2020, and Facebook Testing and Verification Research Award 2018. 
      <br>
      <strong>Matthew Ali Sotoudeh's Bio: </strong> Matthew Sotoudeh is a senior undergraduate student at the University of California, Davis, majoring in Computer Science and Mathematics, where he is a Regents Scholar.
      <br>
      <iframe width="100%" src="https://www.youtube-nocookie.com/embed/NAxXCoVjZjI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>
  </div>
    <!-- Nicolas -->
  <div class="talk" id="nicolas">
    <div class="speakerInfo"> 
                <img alt="Nicolas Papernot" src="{{site.baseurl}}/assets/img/nicolas.png">
      <br>
      <a href="https://www.papernot.fr/" target="_blank">Nicolas Papernot</a> 
      <br>
      <a href="https://www.utoronto.ca/" target="_blank">University of Toronto</a>, <a href="https://vectorinstitute.ai/" target="_blank">Vector Institute</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, January 11th, 2021 @ 1PM.
      <br>
<strong> Talk Title:  What does it mean for ML to be trustworthy? </strong>
     <br>
      <strong> Abstract: </strong> The attack surface of machine learning is large: training data can be poisoned, predictions manipulated using adversarial examples, models exploited to reveal sensitive information contained in training data, etc. This is in large parts due to the absence of security and privacy considerations in the design of ML algorithms. Yet, adversaries have clear incentives to target these systems. Thus, there is a need to ensure that computer systems that rely on ML are trustworthy. Fortunately, we are at a turning point where ML is still being adopted, which creates a rare opportunity to address the shortcomings of the technology before it is widely deployed. Designing secure ML requires that we have a solid understanding as to what we expect legitimate model behavior to look like. We structure our discussion around two directions, which we believe are likely to lead to significant progress. The first encompasses a spectrum of approaches to verification and admission control, which is a prerequisite to enable fail-safe defaults in machine learning systems. The second pursues formal frameworks for security and privacy in machine learning, which we argue should strive to align machine learning goals such as generalization with security and privacy desiderata like robustness or privacy. We illustrate these directions with recent work on adversarial examples, privacy-preserving ML, machine unlearning, and deepfakes.
      <br>
      <strong> Bio: </strong> Nicolas Papernot is an Assistant Professor in the Department of Electrical and Computer Engineering and the Department of Computer Science at the University of Toronto. He is also a faculty member at the Vector Institute where he holds a Canada CIFAR AI Chair, and a faculty affiliate at the Schwartz Reisman Institute. His research interests span the security and privacy of machine learning. Nicolas is a Connaught Researcher and was previously a Google PhD Fellow. His work on differentially private machine learning received a best paper award at ICLR 2017. He serves on the program committees of several conferences including ACM CCS, IEEE S&P, and USENIX Security. He is also the co-author of CleverHans and TensorFlow Privacy, two open-source libraries widely adopted in the technical community to benchmark the security and privacy of machine learning. He earned his Ph.D. at the Pennsylvania State University, working with Prof. Patrick McDaniel. Upon graduating, he spent a year as a research scientist at Google Brain in Úlfar Erlingsson's group.
      <br>
      <iframe width="100%" src="https://www.youtube-nocookie.com/embed/gKUGcWj6M4M" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>
  </div>
  

</div>
