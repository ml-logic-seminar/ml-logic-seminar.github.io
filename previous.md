<h2 style="text-align:center"> Previous Talks </h2>

<div class="talks">
  <!-- Chaowei -->
  <div class="talk" id="chaowei">
        <div class="speakerInfo"> 
            <img alt="Chaowei Xiao" src="{{site.baseurl}}/assets/img/chaowei.jpg">
      <br>
      <a href="http://www-personal.umich.edu/~xiaocw/" target="_blank">Chaowei Xiao</a> 
      <br>
      University of Michigan, NVIDIA Research
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, November 30th, 2020 @ 2PM.
      <br>
<strong> Talk Title: </strong> Deep Learning in Adversarial Environments
     <br>
      <strong> Bio: </strong> Chaowei Xiao is a research scientist at NVIDIA Research. He earned his Ph.D. at the University of Michigan, working with Professor Mingyan Liu. His research interests lie at the intersection of computer  security,  privacy,  and machine  learning. His works have been featured in multiple media outlets, including Wired, Fortune, IEEE SPECTRUM. One of his research outputs is now on display at the Science Museum in London.  He has received the best paper award at Mobicom 2014.
      <br>
      <strong> Slides: </strong> TBA
    </div>
  </div>
    <!-- Aws -->
  <div class="talk" id="aws">
        <div class="speakerInfo"> 
            <img alt="Aws Albarghouthi" src="{{site.baseurl}}/assets/img/aws.jpg">
      <br>
      <a href="http://pages.cs.wisc.edu/~aws/" target="_blank">Aws Albarghouthi</a> 
      <br>
      <a href="https://www.wisc.edu/" target="_blank">University of Wisconsin-Madison</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, December 14th, 2020 @ 4PM.
      <br>
<strong> Talk Title: </strong> A Tale of Two Applications of Verification in Machine Learning
     <br>
      <strong> Abstract: </strong> The rise of machine learning, particularly in the form of deep learning, has created a qualitative shift in our conception of what software is and what software can accomplish. But, of course, it’s not all rainbows and butterflies. Researchers have been hard at work trying to understand the fragility of the machine-learning pipeline: from training to inference, small changes to the input can result in radical changes to the output, which can lead to security, safety, as well as ethical problems. In this talk, I will show how new techniques from software verification can help us reason about, and ensure, robustness of machine-learning techniques against training-time (poisoning) and test-time (adversarial-example) attacks.
<br/>
This talk is based on joint work with Yuhao Zhang, Samuel Drews, and Loris D’Antoni.
      <br>
      <strong> Bio: </strong> Aws Albarghouthi is an assistant professor at the University of Wisconsin-Madison. He studies automated synthesis and verification of programs. He received his PhD from the University of Toronto in 2015. He has received a number of best-paper awards for his work (at FSE, UIST, and FAST), an NSF CAREER award, a Google Faculty Research Award, and Facebook Research Awards. Aws is very excited about his virtual visit to Waterloo.
      <br>
      <strong> Meeting Link: </strong> <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=m9457308fe0c342d8bbbdc1062a2ff5cc" target="_blank">WebEx</a>
    </div>
  </div>


</div>
