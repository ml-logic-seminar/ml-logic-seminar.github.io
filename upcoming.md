<h2 style="text-align:center"> Upcoming Talks </h2>

Join our [group](https://groups.google.com/forum/#!forum/ml_logic_seminar/join 
){:target="_blank"} in order to receive emails with the link for the talks.

<div class="talks">    
  <!-- Vivek -->
  <div class="talk" id="vivek">
        <div class="speakerInfo"> 
            <img alt="Vivek Srikumar" src="{{site.baseurl}}/assets/img/vivek.jpg">
      <br>
      <a href="https://svivek.com/" target="_blank">Vivek Srikumar</a> 
      <br>
      University of Utah
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, April 19th, 2020 @ 4PM.
      <br>
<strong> Talk Title: </strong> Where Neural Networks Fail: The Case for a Little Help from Knowledge
     <br>
      <strong> Abstract: </strong> Today's dominant approach for modeling complex tasks involving human language calls for training neural networks using massive datasets. While the agenda is undeniably successful, we may not have the luxury of annotated data for every task or domain of interest. Reducing dependence on labeled examples may require us to rethink how we supervise models.

In this talk, I will discuss some failures of today's end-to-end trained neural networks. Specifically, I will focus on examples that illustrate their inability to internalize knowledge about the world. Following this, I will describe our work on using knowledge to inform neural networks without introducing additional parameters. Declarative rules stated in logic can be systematically compiled into computation graphs that augment the structure of neural models, and also into regularizers that can use labeled or unlabeled examples. I will present experiments involving text understanding which show that such declaratively constrained neural networks can successfully internalize the information in the rules, providing an easy-to-use mechanism for supervising neural networks that does not involve data annotation.
      <br>
      <strong> Bio: </strong> Vivek Srikumar is associate professor in the School of Computing at the University of Utah. His research lies in the areas of natural language processing and machine learning and has been driven by questions arising from the need to reason about textual data with limited explicit supervision and to scale NLP to large problems. His work has been published in various AI, NLP and machine learning venues and has been recognized by paper awards/honorable mention from EMNLP and CoNLL. His work has been supported by awards from the National Science Foundation, and also from Google, Intel and Verisk. He obtained his Ph.D. from the University of Illinois at Urbana-Champaign in 2013 and was a post-doctoral scholar at Stanford University.
      <br>
      <strong> WebEx: </strong>: <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=mef245a0b016a77518f267c6c90e79fe9" target="_blank">Link</a>
    </div>
  </div>
</div>
