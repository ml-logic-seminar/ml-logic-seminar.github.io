<h2 style="text-align:center"> Upcoming Talks </h2>

Join our [group](https://groups.google.com/forum/#!forum/ml_logic_seminar/join 
){:target="_blank"} in order to receive emails with the link for the talks.

<div class="talks">  
  <!-- Gautam -->
  <div class="talk" id="gautam">
    <div class="speakerInfo"> 
                <img alt="Gautam Kamath" src="{{site.baseurl}}/assets/img/gautam.jpg">
      <br>
      <a href="http://www.gautamkamath.com/" target="_blank">Gautam Kamath</a> 
      <br>
      <a href="https://uwaterloo.ca/" target="_blank">University of Waterloo</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, January 18th, 2021 @ 4PM.
      <br>
<strong> Talk Title: Robustness in unsupervised and supervised machine learning </strong>
     <br>
      <strong> Abstract: </strong> Recently, the need for robust machine learning algorithms has become apparent. Whether due to errors in data collection, model misspecification, or adversarial attacks, contaminated datasets arise in many areas. This is an issue, as existing methods appear to be quite brittle to small amounts of errors. Even more worryingly, these models are being deployed in many security-critical settings, such as self-driving cars, where reliability is an absolute must. In this talk, I will describe a line of work in which we provide provable guarantees for robust machine learning in several fundamental settings. I’ll begin by discussing the problem of robust estimation of mean and covariance of a Gaussian distribution, and how to relax this to distributions with weaker assumptions on the moments. I will then describe how these methods can be used to “robustify” supervised learning algorithms by applying robust mean estimation algorithms to the gradients of the dataset. While theoretically sound, the algorithms are also realizable and efficient, and I will present experimental results on both synthetic and real-world data. Based on joint works with Ilias Diakonikolas, Daniel M. Kane, Jerry Li,
Ankur Moitra, Jacob Steinhardt, and Alistair Stewart.
      <br>
      <strong> Bio: </strong> Gautam Kamath is an Assistant Professor at the David R. Cheriton School of Computer Science at the University of Waterloo. He has a B.S. in Computer Science and Electrical and Computer Engineering from Cornell University, and an M.S. and Ph.D. in Computer Science from the Massachusetts Institute of Technology. His research interests lie in methods for statistics and machine learning, with a focus on challenges which arise in modern data
analysis, including data privacy and robustness. He was a Microsoft Research Fellow, as a part of the Simons-Berkeley Research Fellowship Program at the Simons Institute for the Theory of Computing. He is recipient of an NSERC Discovery Accelerator Supplement, and was awarded the Best Student Presentation Award at the ACM Symposium on Theory of Computing in 2012.
      <br>
      <strong> Meeting Link: </strong><a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=m47a8b947775cdec84fd27d1a56d8bcb3" target="_blank">WebEx</a>
    </div>
  </div>
  
    <!-- Shai Ben-David -->
  <div class="talk" id="shai">
    <div class="speakerInfo"> 
                <img alt="Shai Ben-David" src="{{site.baseurl}}/assets/img/shai.jpeg">
      <br>
      <a href="https://cs.uwaterloo.ca/~shai/" target="_blank">Shai Ben-David</a> 
      <br>
      <a href="https://uwaterloo.ca/" target="_blank">University of Waterloo</a>
    </div>
    <div class="talkInfo">
      <table>
        <tr>
          <th> Date </th>
          <th> Talk Titles </th>
        </tr>
        <tr>
          <td> January 25th @4PM EST </td>
          <td> <strong> Fairness in ML  </strong> </td>
        </tr>
        <tr>
          <td> February 1st @4PM EST </td>
          <td> <strong> Clustering  </strong> </td>
        </tr>
        <tr>
          <td> February 8th @4PM EST </td>
          <td>  <strong> Independence from ZFC result </strong> </td>
        </tr>
      </table>
    <br>
    <strong> Bio: </strong> Shai Ben-David earned his Ph.D. in mathematics from the Hebrew University in Jerusalem and has been a professor of computer science at the Technion (Israel Institute of Technology).  He held visiting faculty positions at the Australian National University, Cornell University, ETH Zurich, TTI Chicago and the Simons Institute at Berkeley. Since 2004 Shai has been a professor at the David Cheriton School of Computer Science at the University of Waterloo. Since 2019 he is also a faculty member at the Toronto Vector institute. In recent years his research focus turned to machine learning theory. Among his notable contributions in that field are pioneering steps in the analysis of domain adaptation, learnability of real-valued functions, and change detection in streaming data. Shai has also made fundamental contributions to the theory of clustering and published seminal works on average-case complexity, competitive analysis of algorithms and alternatives to worst-case complexity.
<br>
      <strong> Highlights: </strong>
      <ul>
        <li> President of the Association for Computational Learning Theory (2009-2012). </li>
        <li> Program chair for the major machine learning theory conferences (COLT and ALT, and frequent area chair for ICML, NIPS and AISTATS). </li>
        <li> Co-authored the textbook “Understanding machine learning: from theory to algorithms” (Cambridge University Press 2015). </li>
        <li> Co-author of Best Paper awards, most recently in NIPS 2018. </li>
        <li> CIFAR AI Chair. </li>
        <li> University Research Chair at U Waterloo </li>
      </ul>
      <br>
      <strong> Meeting Link: </strong> <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=m1d2a9e9b7d8700202dd5facb5a0cdde8" target="_blank">WebEx</a>
    </div>
  </div>
  
   <!-- Gagandeep -->
  <div class="talk" id="gagandeep">
    <div class="speakerInfo"> 
                <img alt="Gagandeep Singh" src="{{site.baseurl}}/assets/img/gagandeep.jpg">
      <br>
      <a href="https://www.sri.inf.ethz.ch/people/gagandeep" target="_blank">Gagandeep Singh</a> 
      <br>
      <a href="https://illinois.edu/" target="_blank">University of Illinois Urbana-Champaign (UIUC)</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, February 15th, 2021 @ 4PM EST.
      <br>
    <strong> Talk Title: Certified Artificial Intelligence </strong>
     <br>
      <strong> Abstract: </strong> Despite surpassing human-level performance in many challenging domains such as vision, planning, and natural sciences, there remain concerns about the fragility of modern data-driven AI systems when applied in the real-world, which poses a threat to their wider adoption. Indeed, obtaining AI systems theoretically guaranteed to be safe and reliable is a fundamental challenge of critical importance. In this talk, Gagandeep will present a path towards addressing this fundamental problem. Specifically, He will introduce new mathematical methods combining convex optimization with the classical abstract interpretation framework that enables scalable and precise logical reasoning about the (potentially infinite number of) behaviors of an AI system (e.g., a deep neural network). He will then show how these methods enable both the creation of state-of-the-art automated verifiers for modern AI systems and the design of new provable training techniques. Finally, He will outline several promising future research directions.
      <br>
      <strong> Bio: </strong> Gagandeep Singh will be starting as a tenure-track Assistant Professor in the Department of Computer Science at the University of Illinois Urbana-Champaign (UIUC) from Fall 2021. He is currently working with VMWare Research. His research interests lie at the intersection of artificial intelligence (AI) and programming languages. His long term goal is to design end-to-end automated formal reasoning tools for real-world systems with both software and AI components such as autonomous vehicles, robots, and AI-powered healthcare devices. Previously, he obtained a Ph.D. in Computer Science from ETH Zurich in 2020, where he designed new scalable and precise automated reasoning methods and tools for programs and deep neural networks.  
      <br>
      <strong> Meeting Link: </strong> <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=mda8ba361f6841fd335ee1121e3046867" target="_blank">WebEx</a>
    </div>
  </div>
  
     <!-- Mikolas -->
  <div class="talk" id="mikolas">
    <div class="speakerInfo"> 
                <img alt="Mikoláš Janota" src="{{site.baseurl}}/assets/img/mikolas.png">
      <br>
      <a href="http://sat.inesc-id.pt/~mikolas/" target="_blank">Mikoláš Janota</a> 
      <br>
      <a href="https://www.ulisboa.pt/en/" target="_blank">University of Lisbon</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, February 22nd, 2021 @ 4PM EST.
      <br>
<strong> Talk Title: TBA  </strong>
     <br>
      <strong> Bio: </strong>  TBA
      <br>
      <strong> Meeting Link: </strong> <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=m89c96846c5f13ccb2d5e7dba1333127c" target="_blank">WebEx</a>
    </div>
  </div>

   <!-- Stanley -->
  <div class="talk" id="stanley">
    <div class="speakerInfo"> 
                <img alt="Stanley Bak" src="{{site.baseurl}}/assets/img/stanley.jpg">
      <br>
      <a href="http://stanleybak.com/" target="_blank">Stanley Bak</a> 
      <br>
      <a href="https://www.cs.stonybrook.edu/" target="_blank">Stony Brook University</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, March 8th, 2021 @ 4PM EST.
      <br>
<strong> Talk Title: Formal Verification for Neural Networks and Cyber-Physical Systems with Reachability Methods </strong>
     <br>
      <strong> Abstract: </strong> Neural Networks have made great advances in vision and game-playing in the last decade. Beyond these core areas, however, almost every other field has at least attempted to use neural networks to some extent. Cyber-physical systems (CPS), where computers interact with the physical world, are no different. Although learning-based controllers have long been looked at in control theory, recent interest in combining CPS with neural networks has soared. One important distinction with CPS, however, is that these systems are not confined to the digital world. Behaviors of a neural network in a CPS may manifest in the physical world. CPS are often safety-critical and cannot tolerate any mistakes.
<br>
The field of formal verification has traditionally looked at proving properties about finite state machines or software programs. Recent research has also looked at other classes of systems such as CPS models given with differential equations, or even systems that include neural networks. This talk reviews some of our research in these areas, as well as results from the first international verification competition for neural networks (VNN-COMP) which took place last July. For a set of neural networks that performs automated aircraft collision avoidance, our nnenum tool was both the fastest and the only one to successfully prove all of the formal properties in the competition.
      <br>
      <strong> Bio: </strong> Stanley Bak is a computer scientist investigating the verification of autonomy, cyber-physical systems, and neural networks. He strives to develop practical formal methods that are both scalable and useful, which demands developing new theory, programming efficient tools and building experimental systems.
<br>
Stanley Bak received a Bachelor's degree in Computer Science from Rensselaer Polytechnic Institute (RPI) in 2007 (summa cum laude), and a Master's degree in Computer Science from the University of Illinois at Urbana-Champaign (UIUC) in 2009. He completed his PhD from the Department of Computer Science at UIUC in 2013. He received the Founders Award of Excellence for his undergraduate research at RPI in 2004, the Debra and Ira Cohen Graduate Fellowship from UIUC twice, in 2008 and 2009, and was awarded the Science, Mathematics and Research for Transformation (SMART) Scholarship from 2009 to 2013. From 2013 to 2018, Stanley was a Research Computer Scientist at the US Air Force Research Lab (AFRL), both in the Information Directorate in Rome, NY, and in the Aerospace Systems Directorate in Dayton, OH. He helped run Safe Sky Analytics, a research consulting company investigating verification and autonomous systems, and performed teaching at Georgetown University before joining Stony Brook University as an assistant professor in Fall 2020.
      <br>
      <strong> Meeting Link: </strong> <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=m7696b29019e9f6cec674ceddc613954b" target="_blank">WebEx</a>
    </div>
  </div>
  
     <!-- Krishnaram -->
  <div class="talk" id="krishnaram">
    <div class="speakerInfo"> 
                <img alt="Stanley Bak" src="{{site.baseurl}}/assets/img/krishnaram.jpg">
      <br>
      <a href="https://www.linkedin.com/in/krishnaramkenthapadi" target="_blank">Krishnaram Kenthapadi</a> 
      <br>
      <a href="https://aws.amazon.com/ai/" target="_blank">Amazon AWS AI</a>
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, April 12th, 2021 @ 4PM EST.
      <br>
<strong> Talk Title: Fairness, Explainability, and Privacy in AI/ML Systems</strong>
     <br>
      <strong> Abstract: </strong> How do we develop machine learning models and systems taking fairness, accuracy, explainability, and transparency into account? How do we protect the privacy of users when building large-scale AI based systems? Model fairness and explainability and protection of user privacy are considered prerequisites for building trust and adoption of AI systems in high stakes domains such as lending and healthcare requiring reliability, safety, and fairness.
<br>
We will first motivate the need for adopting a “fairness, explainability, and privacy by design” approach when developing AI/ML models and systems for different consumer and enterprise applications from the societal, regulatory, customer, end-user, and model developer perspectives. We will then focus on the application of fairness-aware ML, explainable AI, and privacy-preserving AI techniques in practice through industry case studies. We will discuss the sociotechnical dimensions and practical challenges, and conclude with the key takeaways and open challenges.
      <br>
      <strong> Bio: </strong> Krishnaram Kenthapadi is a Principal Scientist at Amazon AWS AI, where he leads the fairness, explainability, and privacy initiatives in Amazon AI platform. Until recently, he led similar efforts at LinkedIn AI team, and served as LinkedIn’s representative in Microsoft’s AI and Ethics in Engineering and Research Advisory Board. Previously, he was a Researcher at Microsoft Research, where his work resulted in product impact (and Gold Star / Technology Transfer awards), and several publications/patents. Krishnaram received his Ph.D. in Computer Science from Stanford University in 2006. He serves regularly on the program committees of KDD, WWW, WSDM, and related conferences, and co-chaired the 2014 ACM Symposium on Computing for Development. His work has been recognized through awards at NAACL, WWW, SODA, CIKM, ICML AutoML workshop, and Microsoft’s AI/ML conference. He has published 40+ papers, with 2500+ citations and filed 140+ patents (30+ granted). He has presented lectures/tutorials on <a href="https://sites.google.com/view/privacy-tutorial" target="_blank">privacy</a>, <a href="https://sites.google.com/view/fairness-tutorial" target="_blank">fairness</a>, and <a href="https://sites.google.com/view/explainable-ai-tutorial" target="_blank">explainable AI</a> at KDD ’18 ’19, WSDM ’19, WWW ’19 ’20, FAccT ’20, and AAAI ’20.
      <br>
      <strong> Meeting Link: </strong> <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=m1400476a87f52ae676f9f07192055634" target="_blank">WebEx</a>
    </div>
  </div>

</div>
