<h2 style="text-align:center"> Upcoming Talks </h2>

Join our [group](https://groups.google.com/forum/#!forum/ml_logic_seminar/join 
){:target="_blank"} in order to receive emails with the link for the talks.

<div class="talks">    
  <!-- Vivek -->
  <div class="talk" id="vivek">
        <div class="speakerInfo"> 
            <img alt="Vivek Srikumar" src="{{site.baseurl}}/assets/img/vivek.jpg">
      <br>
      <a href="https://svivek.com/" target="_blank">Vivek Srikumar</a> 
      <br>
      University of Utah
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, April 19th, 2021 @ 4PM.
      <br>
<strong> Talk Title: </strong> Where Neural Networks Fail: The Case for a Little Help from Knowledge
     <br>
      <strong> Abstract: </strong> Today's dominant approach for modeling complex tasks involving human language calls for training neural networks using massive datasets. While the agenda is undeniably successful, we may not have the luxury of annotated data for every task or domain of interest. Reducing dependence on labeled examples may require us to rethink how we supervise models.
<br>
In this talk, I will discuss some failures of today's end-to-end trained neural networks. Specifically, I will focus on examples that illustrate their inability to internalize knowledge about the world. Following this, I will describe our work on using knowledge to inform neural networks without introducing additional parameters. Declarative rules stated in logic can be systematically compiled into computation graphs that augment the structure of neural models, and also into regularizers that can use labeled or unlabeled examples. I will present experiments involving text understanding which show that such declaratively constrained neural networks can successfully internalize the information in the rules, providing an easy-to-use mechanism for supervising neural networks that does not involve data annotation.
      <br>
      <strong> Bio: </strong> Vivek Srikumar is associate professor in the School of Computing at the University of Utah. His research lies in the areas of natural language processing and machine learning and has been driven by questions arising from the need to reason about textual data with limited explicit supervision and to scale NLP to large problems. His work has been published in various AI, NLP and machine learning venues and has been recognized by paper awards/honorable mention from EMNLP and CoNLL. His work has been supported by awards from the National Science Foundation, and also from Google, Intel and Verisk. He obtained his Ph.D. from the University of Illinois at Urbana-Champaign in 2013 and was a post-doctoral scholar at Stanford University.
      <br>
      <strong> WebEx: </strong>: <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=mef245a0b016a77518f267c6c90e79fe9" target="_blank">Link</a>
    </div>
  </div>
  
   <!-- Pedro -->
  <div class="talk" id="pedro">
        <div class="speakerInfo"> 
            <img alt="Pedro Domingos" src="{{site.baseurl}}/assets/img/pedro.jpg">
      <br>
      <a href="https://homes.cs.washington.edu/~pedrod/" target="_blank">Pedro Domingos</a> 
      <br>
      University of Washington
    </div>
    <div class="talkInfo"> 
              <strong> Date: </strong> Monday, April 26th, 2021 @ 4PM.
      <br>
<strong> Talk Title: </strong> Unifying Logical and Statistical AI with Markov Logic
     <br>
      <strong> Abstract: </strong> Intelligent systems must be able to handle the complexity and uncertainty of the real world. Markov logic enables this by unifying first-order logic and probabilistic graphical models into a single representation. Many deep architectures are instances of Markov logic. An extensive suite of learning and inference algorithms for Markov logic has been developed, along with open source implementations like Alchemy. Markov logic has been applied to natural language understanding, information extraction and integration, robotics, social network analysis, computational biology, and many other areas.
      <br>
      <strong> Bio: </strong> Pedro Domingos is a professor of computer science at the University of Washington and the author of "The Master Algorithm". He is a winner of the SIGKDD Innovation Award and the IJCAI John McCarthy Award, two of the highest honors in data science and AI, and a Fellow of the AAAS and AAAI. His research spans a wide variety of topics in machine learning, artificial intelligence, and data science. He helped start the fields of statistical relational AI, data stream mining, adversarial learning, machine learning for information integration,
and influence maximization in social networks. 
      <br>
      <strong> WebEx: </strong>: <a href="https://uwaterloo.webex.com/uwaterloo/j.php?MTID=m55aa76daf35b43d20adc2bff4c4313ae" target="_blank">Link</a>
    </div>
  </div>
</div>
